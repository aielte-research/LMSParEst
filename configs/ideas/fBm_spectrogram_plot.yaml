experiment_name: ""
experiment_name_base: fBm_Hurst_spectrogram_MLP #base name for the experiments
experiment_name_extension: !!python/tuple ['data_params/n'] # Eg using this experiment_name: example_metrics_a-0_b-1
experiment_tags: !!python/tuple ["fBm","Hurst","MLP"]
#+-------------+
#| DATA PARAMS |
#+-------------+
#the following parameters will be read by the database file (in this case time_series.py)
data_fname: time_series.py
data_params: &data_params
    ts_type: fbm          
    hurst: 
        gen: random.uniform
        a: 0
        b: 1
    epoch_length: 10
    n:  
        - 3200
        - 12800
    lambd: 1                              
    target_param: !!python/tuple ['hurst']
    method: fft

#+--------------+
#| MODEL PARAMS |
#+--------------+
#this is the way to specify which model to train:
model_checkpoint_path: null
model_fname: base.py
model_params:
    diff: true
    standardize: true
    invariant_nu_layer: null
    additive_nu_layer: null
    #base_type: polynomial
    #base_dim: 0
    embedding:
        type: spectrogram
        output_dim: 64
    adaptive_out_length: 1
    vec_hom: null
    mlp:
        bias: true
        channels: !!python/tuple [64, 128, 64, 1]
        batch_norm: false # Add batch normalization between the output mlp layers (but not after the last)
        dropout: 0
        activation:
            name: PReLU
        
    avg_base: null
    nu_estimator: false
    simple_nu_estimator: false
    rs: null
    init_hom: null

#+--------------+
#| TRAIN PARAMS |
#+--------------+
train_params:
    shuffle: false
    freeze_list: !!python/tuple []
    lr: 0.0001
    optimizer_class: torch.optim.AdamW
    loss_fun_class: torch.nn.MSELoss
    
    num_epochs: 1
    train_batch_size: 10
    val_batch_size: 10
    skip_train: false
    skip_val: true

    #? if should be in trainer
    metric_names:

    num_cores: 16
    checkpoint_save_path: null

    metrics:
        session: !!python/dict {}
        experiment: !!python/dict {}
        epoch: 
            epoch_loss:
                child_metric: batch_loss
                metric_func: my_avg
                metric_params:
                    message: loss
        batch:
            batch_real_inferred:
                metric_func: batch_saver
                metric_params:
                    start_count: 0
            batch_spectrogram_plot:
                metric_func: batch_spectrogram_plotter
                metric_params:
                    start_count: 0


        #batch_loss is here by default

    running_logs:
        session: true
        experiment: false
        epoch: false
        batch: false