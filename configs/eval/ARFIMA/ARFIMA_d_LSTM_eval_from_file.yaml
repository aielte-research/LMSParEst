experiment_name: ""
experiment_name_base: ARFIMA_d_LSTM_inference #base name for the experiments
experiment_name_extension: !!python/tuple ['data_params/n'] # Eg using this experiment_name: example_metrics_a-0_b-1
experiment_tags: !!python/tuple ["ARFIMA","d","LSTM","inference"]
#+-------------+
#| DATA PARAMS |
#+-------------+
data_fname: time_series_from_file.py
data_params:
    fpath: null         #The input '.csv', '.tsv', or '.json' file.
    epoch_length: null  #Set by the data contained in the file specified in 'fpath'.
    n: null             #If left 'null' it is determined as the minimum sequence length in data contained in the file specified in 'fpath'. All data will be truncated to this length. Should be less or equal to the minimum sequence lenght provided in the input. 
    target_param_idx: 0 #The target param is at this index in every row. Set to 'null' when no target param is given in the input.
    seq_start_idx: 1    #The sequence starts at this index in every row.
    inference: false    #If true only inference is done, set to True only when 'target_param_idx: null'
    hurst:              #name of the target param and its range (for logging purposes)
        a: 0
        b: 1

#+--------------+
#| MODEL PARAMS |
#+--------------+
#this is the way to specify which model to train:
model_checkpoint_path: model_checkpoints/ARFIMA/ARFIMA_d_LSTM_finetune_until_n-12800.pt
model_fname: LSTM.py
model_params:
    diff: false #work on the increments, instead of the original series
    standardize: false
    invariant_nu_layer: null
    additive_nu_layer: null
    init_hom: null

    embedding: null
        #bias: false
        #channels: !!python/tuple [1, 128, 256, 256, 128]
        #kernel_sizes: 4 
        #activation:
            #name: PReLU
        #leave_last_layer_bare: true

    vec_hom: null

    lstm:
        input_size: 1
        hidden_size: 128
        num_layers: 2
        bidirectional: false # if true the input channel size of the mlp needs to be double of the lstm hidden size!
        residual: false

    adaptive_out_length: 1
    
    mlp:
        bias: true
        channels: !!python/tuple [128, 64, 1]
        batch_norm: false # Add batch normalization between the output mlp layers (but not after the last)
        dropout: 0
        activation:
            name: PReLU

#+--------------+
#| TRAIN PARAMS |
#+--------------+
train_params:
    shuffle: false
    freeze_list: !!python/tuple []
    lr: 0.0001
    optimizer_class: torch.optim.AdamW
    loss_fun_class: torch.nn.MSELoss
    
    num_epochs: 1
    train_batch_size: 32
    val_batch_size: 32
    skip_train: true
    skip_val: false

    #? if should be in trainer
    metric_names:
    change_seed: true
    num_cores: 16
    checkpoint_save_path: null

    metrics:
        session:
            export_results: 
                child_metric: exp_real_inferred
                metric_func: export_results
                metric_params: 
                    fpath: null
                    export_targets: true
                    precision: 17

            scatter_plot: 
                child_metric: exp_real_inferred
                metric_func: scatter_plot
                metric_params:
                    title: d

            deviation_plot: 
                child_metric: exp_real_inferred
                metric_func: deviation
                metric_params:
                    plot_limit: 50000
                    measure_interval: 0.025
                    steps: 1000
                    title: d

            loss_plot: 
                child_metric: exp_loss
                metric_func: general_line_plot
                metric_params:
                    title: ARFIMA d losses

            mse_plot: 
                child_metric: exp_real_inferred
                metric_func: mse
                metric_params:
                    title: d
                
        experiment:
            exp_real_inferred:
                child_metric: ep_real_inferred
                metric_func: experiment_saver
                metric_params: !!python/dict {}

            exp_loss:
                child_metric: epoch_loss
                metric_func: experiment_saver
                metric_params: !!python/dict {}

        epoch:
            epoch_loss:
                child_metric: batch_loss
                metric_func: my_avg
                metric_params:
                    message: loss

            ep_real_inferred:
                child_metric: batch_real_inferred
                metric_func: epoch_saver
                metric_params: 
                    start_count: 0
                    overwrite_last: true

        batch:
            batch_real_inferred:
                metric_func: batch_saver
                metric_params:
                    start_count: 0

        #batch_loss is here by default

    running_logs:
        session: true
        experiment: false
        epoch: false
        batch: false
