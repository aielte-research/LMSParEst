import sys
sys.path.append('../')
from metrics.plotters import general_plot

## Comparing models trained on different n-s
#PEFBM-4965 - PEFBM-5013

# FBM_nxn_Ys=[
#     [0.00920837, 0.00443827, 0.00256292, 0.00171017, 0.00130304, 0.00108711, 0.000981199, 0.000930546],
#     [0.0095864, 0.00413592, 0.0021675, 0.00140661, 0.00109023, 0.000962041, 0.000917925, 0.000915043],
#     [0.0104301, 0.00420707, 0.00188037, 0.000947045, 0.000527753, 0.000343587, 0.000264268, 0.000230696],
#     [0.012502, 0.00477969, 0.00202007, 0.000939761, 0.000477126, 0.000280757, 0.000195706, 0.000160969],
#     [0.012518, 0.00480479, 0.00199526, 0.000912694, 0.000443194, 0.00022961, 0.000133505, 0.000088832],
#     [0.0129322, 0.00501386, 0.0021069, 0.000952266, 0.00044714, 0.000220096, 0.000112802, 0.000061733],
#     [0.0141555, 0.00544181, 0.00222571, 0.000971535, 0.000453955, 0.000220652, 0.00011109, 0.0000607797]
# ]
# print("$N = 50$ & "+"\\\\\n\\hline\n$N = 50$ & ".join([" & ".join([f"${y*1000:#.3g}$" for y in Y]) for Y in FBM_nxn_Ys])+"\\\\")

FBM_nxn_Ys=[
    [None, 0.0276449, 0.0189365, 0.0138884, 0.0108062, 0.00862036, 0.00673888, 0.00557211],
    [None, 0.00930144, 0.00505078, 0.00291788, 0.00174502, 0.00109185, 0.000723842, 0.000502304],
    [None, 0.0105847, 0.00420909, 0.0019889, 0.00104599, 0.000593053, 0.00036006, 0.000228544],
    [None, 0.0043292, 0.00199614, 0.00100069, 0.000540158, 0.000324259, 0.000225005, 0.00017898],
    #[0.00920837, 0.00443827, 0.00256292, 0.00171017, 0.00130304, 0.00108711, 0.000981199, 0.000930546],
    [0.0095864, 0.00413592, 0.0021675, 0.00140661, 0.00109023, 0.000962041, 0.000917925, 0.000915043],
    [0.0104301, 0.00420707, 0.00188037, 0.000947045, 0.000527753, 0.000343587, 0.000264268, 0.000230696],
    [0.012502, 0.00477969, 0.00202007, 0.000939761, 0.000477126, 0.000280757, 0.000195706, 0.000160969],
    [0.012518, 0.00480479, 0.00199526, 0.000912694, 0.000443194, 0.00022961, 0.000133505, 0.000088832],
    [0.0129322, 0.00501386, 0.0021069, 0.000952266, 0.00044714, 0.000220096, 0.000112802, 0.000061733],
    [0.0141555, 0.00544181, 0.00222571, 0.000971535, 0.000453955, 0.000220652, 0.00011109, 0.0000607797],
    [None, 0.00478357, 0.00205344, 0.000942329, 0.000451415, 0.000221565, 0.000109246, 0.000056241]
]

general_plot({
    "Ys": [Y[1:] for Y in FBM_nxn_Ys[4:]],
    "Xs": [100,200,400,800,1600,3200,6400],
    "xlabel": "Sequence Length (n)",
    "xscale": "log",
    "ylabel": "MSE Loss",
    "yscale": "log",
    "title": "", #"Performance of models trained on different sequence lengths",
    "fname": "fBm_Hurst_loss_by_n_by_n_loglog",
    "dirname": "./plots",
    "markers": None,
    "legend": {
        "location": "bottom_left",
        "labels": ["R/S","variogram","Higuchi","Whittle","LSTM trained on n=100",
                   "LSTM trained on n=200","LSTM trained on n=400","LSTM trained on n=800",
                   "LSTM trained on n=1600","LSTM trained on n=3200","LSTM fine-tuned until n=12800"][4:]#,"Our model with transformer encoder"]
    },
    "matplotlib": {
        "calc_xtics": False,
        "width": 6.75,
        "height": 4.025,
        "style": "default"
    },
    "colors": ["red","red","red","red","#003193","#0053A8","#007185","#009658","#00BA2D","#00D40E","black"][4:],
    #"colors": ["red","red","#FED303","#FFF54E","#B2E782","#8FD79F","#70C4BC","#4292B9"],
    "dashes": ["solid","dashdot","dashed","dotted","solid","solid","solid","solid","solid","solid","solid"][4:],
    "color_settings":{
        "bg_transparent": False
    }
})